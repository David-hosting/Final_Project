# -*- coding: utf-8 -*-
"""Models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nKqcPxM9vzLN-x6JmnVvNi2H4hGs1vtb

# **Models for my Final Project - "IntelliFleetManager"**
"""

# For connection to database
import mysql.connector

# For organizing the data
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt

# For model 1 + 2
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# For model 3
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules


# -----------------------------------------------------------------------------------------------------------------------------------------------#

"""# **Model 1 : Predicts if there is an issue or not**"""

# -----------------------------------------------------------------------------------------------------------------------------------------------#


def model1(data):
    x1, y1 = model1_data(data)
    x_train1, x_test1, y_train1, y_test1 = model1_split_data(x1, y1)
    model1_train(x_train1, x_test1, y_train1, y_test1)


def model1_data(data):
    # Organize the data into x1, y1
    x1 = data.drop(['issues', 'trouble_codes', 'time', 'vehicle_id', 'id', 'ip'], axis=1).values
    y1 = data['issues'].values

    # Print
    print(x1)
    print(y1)

    return x1, y1


def model1_split_data(x1, y1):
    # Split the data into training and testing sets
    X_train1, X_test1, y_train1, y_test1 = train_test_split(x1, y1, test_size=0.2, random_state=42)

    return X_train1, X_test1, y_train1, y_test1


def model1_structure(input_size):
    # Build a neural network model
    model_struct = tf.keras.Sequential([
        tf.keras.layers.Dense(64, input_shape=(input_size,), activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])

    # Compile the model
    model_struct.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Print the model summary
    model_struct.summary()

    # Save model structure
    model_struct.save('../generated/model1_structure.h5')

    return model_struct


def model1_train(x_train1, x_test1, y_train1, y_test1):
    # Build model structure
    model_1 = model1_structure(x_train1.shape[1])

    # Train the model
    history1 = model_1.fit(x_train1, y_train1, epochs=5, batch_size=32, verbose=1, validation_data=(x_test1, y_test1))

    # Save weights
    model_1.save_weights('../generated/model1_weights.hdf5')

    # Evaluate the performance of the model
    loss1, accuracy1 = model_1.evaluate(x_test1, y_test1)
    print('Accuracy:', accuracy1)

    # Plot the training and validation loss and accuracy
    plt.plot(history1.history['loss'], label='train_loss')
    plt.plot(history1.history['val_loss'], label='val_loss')
    plt.plot(history1.history['accuracy'], label='train_acc')
    plt.plot(history1.history['val_accuracy'], label='val_acc')
    plt.legend()
    plt.show()

    y_pred = model_1.predict(x_test1)
    y_pred = np.round(y_pred).astype(int)
    print(y_pred)


# -----------------------------------------------------------------------------------------------------------------------------------------------#

"""# **Model 2 : Predicts which issue**"""

# -----------------------------------------------------------------------------------------------------------------------------------------------#


def model2(data):
    x2, y2 = model2_data(data)
    x_train2, x_test2, y_train2, y_test2 = model1_split_data(x2, y2)
    model2_train(x_train2, x_test2, y_train2, y_test2)


def model2_data(data):
    # Filter the dataset to include only rows where "issues" equals 1
    x2 = data[data['issues'] == 1].copy()

    # Make a y data based on X
    temp_y2 = x2['trouble_codes'].values

    # One-hot encode the labels
    one_hot_encoder = OneHotEncoder(sparse=False)
    temp_y2 = one_hot_encoder.fit_transform(temp_y2.reshape(-1, 1))
    y2 = np.argmax(temp_y2, axis=1)  # Convert one-hot encoded labels to integer labels

    # Drop the columns that we don't need
    x2.drop(['issues', 'trouble_codes', 'time', 'vehicle_id', 'id', 'ip'], axis=1, inplace=True)

    # Print
    print(x2)
    print(y2)

    return x2, y2


def model2_split_data(x2, y2):
    # Split the data into training and testing sets
    x_train2, x_test2, y_train2, y_test2 = train_test_split(x2, y2, test_size=0.2, random_state=42)

    return x_train2, x_test2, y_train2, y_test2


def model2_structure(input_size, output_size):
    # Define the model
    model_struct = tf.keras.Sequential([
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True),
                                      input_shape=(input_size, 1)),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(output_size, activation='softmax')
    ])

    # Compile the model
    model_struct.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Print the model summary
    model_struct.summary()

    # Save model structure
    model_struct.save('../generated/model2_structure.h5')

    return model_struct


def model2_train(x_train2, x_test2, y_train2, y_test2):
    # Get the number of categories
    num_categories = len(np.unique(y_train2))

    # Build model structure
    model_2 = model2_structure(x_train2.shape[1], num_categories)

    # Train the model
    history2 = model_2.fit(np.expand_dims(x_train2, axis=2), y_train2, epochs=5, batch_size=32, verbose=1)

    # Save model weights
    model_2.save_weights('../generated/model2_weights.hdf5')

    # Evaluate the performance of the model
    loss2, accuracy2 = model_2.evaluate(np.expand_dims(x_test2, axis=2), y_test2)
    print('Accuracy:', accuracy2)

    # Plot the training and validation loss and accuracy
    plt.plot(history2.history['loss'], label='train_loss')
    plt.plot(history2.history['accuracy'], label='train_acc')
    plt.legend()
    plt.show()

    y_pred = model_2.predict(x_test2)
    print(y_pred)


# -----------------------------------------------------------------------------------------------------------------------------------------------#

"""# **Model 3 : Predicts which issue will appear next**"""

# -----------------------------------------------------------------------------------------------------------------------------------------------#


def model3(data):
    trouble_data = model3_data(data)
    model3_train(trouble_data)


def model3_data(data):
    # Filter for rows where "issues" == 1
    filtered_data = data[data["issues"] == 1].copy()

    # Create a pivot table to get all unique trouble codes for each car ID
    pivot_table = filtered_data.pivot_table(index=["vehicle_id"], values=["trouble_codes"],
                                            aggfunc=lambda x: tuple(np.unique(x)))

    # Convert the pivot table to a NumPy array
    trouble_data = pivot_table.to_numpy()

    # Print the list of arrays
    print(trouble_data)

    return trouble_data


def model3_train(trouble_data):
    # Convert the dataset into a pandas DataFrame
    dataset = pd.DataFrame(trouble_data)

    # Print the dataset
    print(dataset)

    # Transform the dataset into a one-hot encoded matrix
    onehot = pd.get_dummies(dataset.apply(pd.Series).stack()).sum(level=0)

    # Generate frequent item_sets using the Apriori algorithm
    frequent_item_sets = apriori(onehot, min_support=0.3, use_colnames=True)

    # Generate association rules from frequent itemsets
    rules = association_rules(frequent_item_sets, metric="lift", min_threshold=1)

    # Print the association rules
    print(rules)

    return rules


# -----------------------------------------------------------------------------------------------------------------------------------------------#

"""# **Main**"""

# -----------------------------------------------------------------------------------------------------------------------------------------------#


def get_data():
    mydb = mysql.connector.connect(
        host="localhost",
        user="root",
        password="admin",
        database="obd2data"  # Replace with the name of the database you want to use
    )

    my_cursor = mydb.cursor()

    my_cursor.execute("SELECT * FROM obd2data")

    my_result = my_cursor.fetchall()

    df = pd.DataFrame(my_result, columns=my_cursor.column_names)

    return df


def main():
    # Load the dataset
    data = get_data()
    # data = pd.read_csv('obd2data.csv')

    # Start models
    model1(data)
    model2(data)
    model3(data)

    # model_2.load_weights('model2_weights.hdf5')
    # model_2.scripts()


if __name__ == '__main__':
    main()


